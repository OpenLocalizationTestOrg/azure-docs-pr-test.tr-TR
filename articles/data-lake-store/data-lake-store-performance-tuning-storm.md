---
title: "aaaAzure Data Lake deposu Storm performans yönergeleri ayarlama | Microsoft Docs"
description: "Azure Data Lake Store Storm performans kuralları ayarlama"
services: data-lake-store
documentationcenter: 
author: stewu
manager: amitkul
editor: stewu
ms.assetid: ebde7b9f-2e51-4d43-b7ab-566417221335
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 12/19/2016
ms.author: stewu
ms.openlocfilehash: 5412fd46cf2373f5877030913df4fe1fc6f5473a
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: tr-TR
ms.lasthandoff: 10/06/2017
---
# <a name="performance-tuning-guidance-for-storm-on-hdinsight-and-azure-data-lake-store"></a><span data-ttu-id="a7fd0-103">Performans Kılavuzu Hdınsight ve Azure Data Lake Store üzerinde Storm için ayarlama</span><span class="sxs-lookup"><span data-stu-id="a7fd0-103">Performance tuning guidance for Storm on HDInsight and Azure Data Lake Store</span></span>

<span data-ttu-id="a7fd0-104">Bir Azure Storm topolojisinin hello performansı ayarlamak yükleyen düşünülmesi gereken hello Etkenler anlayın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-104">Understand hello factors that should be considered when you tune hello performance of an Azure Storm topology.</span></span> <span data-ttu-id="a7fd0-105">Örneğin, önemli toounderstand hello hello spout'lar ve hello Cıvatalar (Merhaba iş g/ç yoğun olup) tarafından yapılan hello iş özelliklerini olur.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-105">For example, it's important toounderstand hello characteristics of hello work done by hello spouts and hello bolts (whether hello work is I/O or memory intensive).</span></span> <span data-ttu-id="a7fd0-106">Bu makalede, performans ayarlama yönergeleri, ortak sorunları da dahil olmak üzere çeşitli yer almaktadır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-106">This article covers a range of performance tuning guidelines, including troubleshooting common issues.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="a7fd0-107">Ön koşullar</span><span class="sxs-lookup"><span data-stu-id="a7fd0-107">Prerequisites</span></span>

* <span data-ttu-id="a7fd0-108">**Bir Azure aboneliği**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-108">**An Azure subscription**.</span></span> <span data-ttu-id="a7fd0-109">Bkz. [Azure ücretsiz deneme sürümü alma](https://azure.microsoft.com/pricing/free-trial/).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-109">See [Get Azure free trial](https://azure.microsoft.com/pricing/free-trial/).</span></span>
* <span data-ttu-id="a7fd0-110">**Bir Azure Data Lake Store hesabı**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-110">**An Azure Data Lake Store account**.</span></span> <span data-ttu-id="a7fd0-111">Yönergeler için toocreate bir, bkz: [Azure Data Lake Store ile çalışmaya başlama](data-lake-store-get-started-portal.md).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-111">For instructions on how toocreate one, see [Get started with Azure Data Lake Store](data-lake-store-get-started-portal.md).</span></span>
* <span data-ttu-id="a7fd0-112">**Azure Hdınsight kümesi** erişim tooa Data Lake Store hesabı ile.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-112">**An Azure HDInsight cluster** with access tooa Data Lake Store account.</span></span> <span data-ttu-id="a7fd0-113">Bkz: [Data Lake Store ile bir Hdınsight kümesi oluşturmayı](data-lake-store-hdinsight-hadoop-use-portal.md).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-113">See [Create an HDInsight cluster with Data Lake Store](data-lake-store-hdinsight-hadoop-use-portal.md).</span></span> <span data-ttu-id="a7fd0-114">Merhaba küme için Uzak Masaüstü etkinleştirdiğinizden emin olun.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-114">Make sure you enable Remote Desktop for hello cluster.</span></span>
* <span data-ttu-id="a7fd0-115">**Data Lake Store üzerinde Storm kümesi çalıştıran**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-115">**Running a Storm cluster on Data Lake Store**.</span></span> <span data-ttu-id="a7fd0-116">Daha fazla bilgi için bkz: [Hdınsight üzerinde Storm](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-116">For more information, see [Storm on HDInsight](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview).</span></span>
* <span data-ttu-id="a7fd0-117">**Performans ayarlama yönergeleri Data Lake Store üzerinde**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-117">**Performance tuning guidelines on Data Lake Store**.</span></span>  <span data-ttu-id="a7fd0-118">Genel performans için bkz [Data Lake deposu performans ayarlama Kılavuzu](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-118">For general performance concepts, see [Data Lake Store Performance Tuning Guidance](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance).</span></span>  

## <a name="tune-hello-parallelism-of-hello-topology"></a><span data-ttu-id="a7fd0-119">Merhaba paralellik hello topolojisinin ayarlama</span><span class="sxs-lookup"><span data-stu-id="a7fd0-119">Tune hello parallelism of hello topology</span></span>

<span data-ttu-id="a7fd0-120">Data Lake Store gelen hello g/ç tooand artan hello eşzamanlılığı tarafından mümkün tooimprove performans olabilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-120">You might be able tooimprove performance by increasing hello concurrency of hello I/O tooand from Data Lake Store.</span></span> <span data-ttu-id="a7fd0-121">Bir Storm topolojisinin hello paralellik belirlemek yapılandırmaları kümesi vardır:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-121">A Storm topology has a set of configurations that determine hello parallelism:</span></span>
* <span data-ttu-id="a7fd0-122">(Merhaba VM'ler arasında hello çalışanları olacak şekilde eşit dağıtılır) çalışan işlem sayısı.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-122">Number of worker processes (hello workers are evenly distributed across hello VMs).</span></span>
* <span data-ttu-id="a7fd0-123">Spout Yürütücü örneği sayısı.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-123">Number of spout executor instances.</span></span>
* <span data-ttu-id="a7fd0-124">Cıvata Yürütücü örneği sayısı.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-124">Number of bolt executor instances.</span></span>
* <span data-ttu-id="a7fd0-125">Spout görev sayısı.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-125">Number of spout tasks.</span></span>
* <span data-ttu-id="a7fd0-126">Cıvata görev sayısı.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-126">Number of bolt tasks.</span></span>

<span data-ttu-id="a7fd0-127">Örneğin, 4 VM'ler ve 4 çalışan işlemleri, 32 spout yürütücüler 32 spout görevleri ve 256 Cıvata yürütücüler ve 512 Cıvata görevleri ile bir kümede hello aşağıdakileri göz önünde bulundurun:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-127">For example, on a cluster with 4 VMs and 4 worker processes, 32 spout executors and 32 spout tasks, and 256 bolt executors and 512 bolt tasks, consider hello following:</span></span>

<span data-ttu-id="a7fd0-128">Çalışan düğümüne olan her yönetici tek bir çalışan Java sanal makine (JVM) işlemi vardır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-128">Each supervisor, which is a worker node, has a single worker Java virtual machine (JVM) process.</span></span> <span data-ttu-id="a7fd0-129">Bu JVM işlem 4 spout iş parçacıkları ve 64 Cıvata iş parçacığı yönetir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-129">This JVM process manages 4 spout threads and 64 bolt threads.</span></span> <span data-ttu-id="a7fd0-130">Her iş parçacığı içinde görevleri sırayla çalışır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-130">Within each thread, tasks are run sequentially.</span></span> <span data-ttu-id="a7fd0-131">Yapılandırma önceki hello ile 1 görev her spout iş parçacığı vardır ve her Cıvata iş parçacığı 2 görevleri vardır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-131">With hello preceding configuration, each spout thread has 1 task, and each bolt thread has 2 tasks.</span></span>

<span data-ttu-id="a7fd0-132">Storm, ilgili çeşitli bileşenleri hello işte ve sahip olduğunuz paralellik hello düzeyini nasıl etkilediklerini:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-132">In Storm, here are hello various components involved, and how they affect hello level of parallelism you have:</span></span>
* <span data-ttu-id="a7fd0-133">Merhaba baş düğümü (Storm içinde çağrılan Nimbus) kullanılan toosubmit olduğu ve işleri yönetin.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-133">hello head node (called Nimbus in Storm) is used toosubmit and manage jobs.</span></span> <span data-ttu-id="a7fd0-134">Bu düğümler üzerinde hello paralellik derecesi herhangi bir etkisi vardır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-134">These nodes have no impact on hello degree of parallelism.</span></span>
* <span data-ttu-id="a7fd0-135">Hello Yöneticisi düğümleri.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-135">hello supervisor nodes.</span></span> <span data-ttu-id="a7fd0-136">Hdınsight'ta, bu tooa çalışan düğümüne Azure VM karşılık gelir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-136">In HDInsight, this corresponds tooa worker node Azure VM.</span></span>
* <span data-ttu-id="a7fd0-137">Merhaba VM'ler çalışan Storm işlemler Hello çalışan görevlerdir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-137">hello worker tasks are Storm processes running in hello VMs.</span></span> <span data-ttu-id="a7fd0-138">Her çalışan görev tooa JVM örneği karşılık gelir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-138">Each worker task corresponds tooa JVM instance.</span></span> <span data-ttu-id="a7fd0-139">Storm dağıtır hello numarası çalışan işlemleri, toohello çalışan düğümleri mümkün olduğunca eşit belirtin.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-139">Storm distributes hello number of worker processes you specify toohello worker nodes as evenly as possible.</span></span>
* <span data-ttu-id="a7fd0-140">Spout ve yürütücü örnekleri Cıvata.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-140">Spout and bolt executor instances.</span></span> <span data-ttu-id="a7fd0-141">Her bir yürütücü örneği hello çalışanları (JVMs) içinde çalışan tooa iş parçacığı karşılık gelir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-141">Each executor instance corresponds tooa thread running within hello workers (JVMs).</span></span>
* <span data-ttu-id="a7fd0-142">Storm görevler.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-142">Storm tasks.</span></span> <span data-ttu-id="a7fd0-143">Bunların her birini Çalıştır iş parçacıkları mantıksal görevlerdir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-143">These are logical tasks that each of these threads run.</span></span> <span data-ttu-id="a7fd0-144">Yürütücü başına birden çok görev veya gerekiyorsa değerlendirmelidir şekilde bu paralellik, hello düzeyini değiştirmez.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-144">This does not change hello level of parallelism, so you should evaluate if you need multiple tasks per executor or not.</span></span>

### <a name="get-hello-best-performance-from-data-lake-store"></a><span data-ttu-id="a7fd0-145">Data Lake Deposu'ndan veri Hello en iyi performansı elde</span><span class="sxs-lookup"><span data-stu-id="a7fd0-145">Get hello best performance from Data Lake Store</span></span>

<span data-ttu-id="a7fd0-146">Aşağıdaki Merhaba, Data Lake Store ile çalışırken, hello en iyi performansı elde:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-146">When working with Data Lake Store, you get hello best performance if you do hello following:</span></span>
* <span data-ttu-id="a7fd0-147">Birleşim küçük ve büyük boyutları (İdeal olarak 4 MB) ekler.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-147">Coalesce your small appends into larger sizes (ideally 4 MB).</span></span>
* <span data-ttu-id="a7fd0-148">Mümkün olduğu kadar eşzamanlı istek yapın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-148">Do as many concurrent requests as you can.</span></span> <span data-ttu-id="a7fd0-149">Her Cıvata iş parçacığı engelleme okuma yapmak için herhangi bir yerde aralığındaki hello çekirdek başına 8-12 iş parçacığı toohave istiyorsunuz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-149">Because each bolt thread is doing blocking reads, you want toohave somewhere in hello range of 8-12 threads per core.</span></span> <span data-ttu-id="a7fd0-150">Bu da kullanılan hello NIC ve hello CPU tutar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-150">This keeps hello NIC and hello CPU well utilized.</span></span> <span data-ttu-id="a7fd0-151">Daha büyük bir VM daha fazla istek sağlar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-151">A larger VM enables more concurrent requests.</span></span>  

### <a name="example-topology"></a><span data-ttu-id="a7fd0-152">Örnek topoloji</span><span class="sxs-lookup"><span data-stu-id="a7fd0-152">Example topology</span></span>

<span data-ttu-id="a7fd0-153">Bir 8 çalışan düğümü küme D13v2 Azure VM ile sahip varsayalım.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-153">Let’s assume you have an 8 worker node cluster with a D13v2 Azure VM.</span></span> <span data-ttu-id="a7fd0-154">Bu VM, 8 çekirdek sahip, bu nedenle arasında 8 çalışan düğümleri Merhaba, 64 toplam çekirdeğe sahip.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-154">This VM has 8 cores, so among hello 8 worker nodes, you have 64 total cores.</span></span>

<span data-ttu-id="a7fd0-155">Biz çekirdek başına 8 Cıvata iş parçacığı yapmak varsayalım.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-155">Let’s say we do 8 bolt threads per core.</span></span> <span data-ttu-id="a7fd0-156">64 çekirdek, 512 toplam Cıvata Yürütücü örnekleri (diğer bir deyişle, iş parçacıkları) istiyoruz anlamına gelir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-156">Given 64 cores, that means we want 512 total bolt executor instances (that is, threads).</span></span> <span data-ttu-id="a7fd0-157">Bu durumda, biz VM başına bir JVM başlayın ve çoğunlukla hello iş parçacığı eşzamanlılık hello JVM tooachieve eşzamanlılık içinde kullanmak varsayalım.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-157">In this case, let’s say we start with one JVM per VM, and mainly use hello thread concurrency within hello JVM tooachieve concurrency.</span></span> <span data-ttu-id="a7fd0-158">8 çalışan görevler (Azure VM başına bir tane) ve 512 Cıvata yürütücüler ihtiyacımız anlamına gelir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-158">That means we need 8 worker tasks (one per Azure VM), and 512 bolt executors.</span></span> <span data-ttu-id="a7fd0-159">Bu yapılandırma verildiğinde, Storm her bir çalışan düğümünün 1 vermiş toodistribute hello çalışanları çalışan düğümleri (olarak da bilinen yönetici düğümler), arasında eşit olarak çalıştığında JVM.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-159">Given this configuration, Storm tries toodistribute hello workers evenly across worker nodes (also known as supervisor nodes), giving each worker node 1 JVM.</span></span> <span data-ttu-id="a7fd0-160">Merhaba denetçiler içinde Storm toodistribute hello yürütücüler denetçiler arasında eşit olarak çalışır. Şimdi her yönetici (diğer bir deyişle, JVM) vermiş 8 her iş parçacıkları.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-160">Now within hello supervisors, Storm tries toodistribute hello executors evenly between supervisors, giving each supervisor (that is, JVM) 8 threads each.</span></span>

## <a name="tune-additional-parameters"></a><span data-ttu-id="a7fd0-161">Ek parametrelerini ayarlama</span><span class="sxs-lookup"><span data-stu-id="a7fd0-161">Tune additional parameters</span></span>
<span data-ttu-id="a7fd0-162">Merhaba temel topoloji aldıktan sonra tootweak hello parametrelerinden herhangi birini isteyip istemediğinizi düşünebilirsiniz:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-162">After you have hello basic topology, you can consider whether you want tootweak any of hello parameters:</span></span>
* <span data-ttu-id="a7fd0-163">**Çalışan düğüm başına JVMs sayısı.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-163">**Number of JVMs per worker node.**</span></span> <span data-ttu-id="a7fd0-164">Büyük veri yapısı (örneğin, bir arama tablosu) varsa, barındıran bellekte her JVM ayrı bir kopyasını gerektirir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-164">If you have a large data structure (for example, a lookup table) that you host in memory, each JVM requires a separate copy.</span></span> <span data-ttu-id="a7fd0-165">Alternatif olarak, daha az JVMs varsa birçok iş parçacıkları arasında hello veri yapısı kullanabilirsiniz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-165">Alternatively, you can use hello data structure across many threads if you have fewer JVMs.</span></span> <span data-ttu-id="a7fd0-166">Merhaba Cıvata'nın g/ç JVMs hello sayısı kadar hello bu JVMs eklenen iş parçacığı sayısı olarak farkının yapmaz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-166">For hello bolt’s I/O, hello number of JVMs does not make as much of a difference as hello number of threads added across those JVMs.</span></span> <span data-ttu-id="a7fd0-167">İsteğe bağlı olarak kolaylık sağlamak için bir fikir toohave biri olan çalışan başına JVM.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-167">For simplicity, it's a good idea toohave one JVM per worker.</span></span> <span data-ttu-id="a7fd0-168">Bu sayı toochange gerekebilir rağmen Cıvata yaptıklarını veya hangi uygulama, işleme bağlı olarak, gerektirir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-168">Depending on what your bolt is doing or what application processing you require, though, you may need toochange this number.</span></span>
* <span data-ttu-id="a7fd0-169">**Spout yürütücüler sayısı.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-169">**Number of spout executors.**</span></span> <span data-ttu-id="a7fd0-170">Merhaba önceki örnekte Cıvatalar tooData Lake Store yazmak için kullandığı için hello spout'lar değil doğrudan ilgili toohello Cıvata performans sayısıdır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-170">Because hello preceding example uses bolts for writing tooData Lake Store, hello number of spouts is not directly relevant toohello bolt performance.</span></span> <span data-ttu-id="a7fd0-171">Ancak, hello miktarda işleme veya gerçekleştiği hello spout iyi bir fikirdir g/ç bağlı olarak, en iyi performans için tootune hello spout'lar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-171">However, depending on hello amount of processing or I/O happening in hello spout, it's a good idea tootune hello spouts for best performance.</span></span> <span data-ttu-id="a7fd0-172">Yeterli spout'lar toobe mümkün tookeep hello Cıvatalar meşgul olduğundan emin olun.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-172">Ensure that you have enough spouts toobe able tookeep hello bolts busy.</span></span> <span data-ttu-id="a7fd0-173">Merhaba spout'lar Hello çıkış oranları hello Cıvatalar hello verimini eşleşmelidir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-173">hello output rates of hello spouts should match hello throughput of hello bolts.</span></span> <span data-ttu-id="a7fd0-174">Merhaba gerçek yapılandırma üzerinde hello spout bağlıdır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-174">hello actual configuration depends on hello spout.</span></span>
* <span data-ttu-id="a7fd0-175">**Görev sayısı.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-175">**Number of tasks.**</span></span> <span data-ttu-id="a7fd0-176">Her Cıvata tek bir iş parçacığı çalıştırır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-176">Each bolt runs as a single thread.</span></span> <span data-ttu-id="a7fd0-177">Ek görevler Cıvata başına herhangi bir ek eşzamanlılık sağlamaz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-177">Additional tasks per bolt don't provide any additional concurrency.</span></span> <span data-ttu-id="a7fd0-178">yararı oldukları hello yalnızca, işleminin hello tuple aktarımının Cıvata yürütme süresi büyük bir kısmının alır, saattir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-178">hello only time they are of benefit is if your process of acknowledging hello tuple takes a large proportion of your bolt execution time.</span></span> <span data-ttu-id="a7fd0-179">Merhaba Cıvata onay göndermeden önce birçok başlıkları daha geniş bir içine ekleme iyi bir fikir toogroup olur.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-179">It's a good idea toogroup many tuples into a larger append before you send an acknowledgement from hello bolt.</span></span> <span data-ttu-id="a7fd0-180">Bu nedenle, çoğu durumda, birden çok görevler ek fayda sağlar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-180">So, in most cases, multiple tasks provide no additional benefit.</span></span>
* <span data-ttu-id="a7fd0-181">**Yerel veya karışık gruplandırma.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-181">**Local or shuffle grouping.**</span></span> <span data-ttu-id="a7fd0-182">Bu ayar etkinleştirildiğinde, tanımlama grupları toobolts hello içinde gönderilen aynı çalışan işlemi.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-182">When this setting is enabled, tuples are sent toobolts within hello same worker process.</span></span> <span data-ttu-id="a7fd0-183">Bu işlemler arası iletişim ve ağ çağrıları azaltır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-183">This reduces inter-process communication and network calls.</span></span> <span data-ttu-id="a7fd0-184">Bu, çoğu Topolojileri için önerilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-184">This is recommended for most topologies.</span></span>

<span data-ttu-id="a7fd0-185">Bu temel senaryo iyi bir başlangıç noktasıdır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-185">This basic scenario is a good starting point.</span></span> <span data-ttu-id="a7fd0-186">Kendi veri tootweak hello parametreleri tooachieve en iyi performans önceki sınayın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-186">Test with your own data tootweak hello preceding parameters tooachieve optimal performance.</span></span>

## <a name="tune-hello-spout"></a><span data-ttu-id="a7fd0-187">Merhaba spout ayarlama</span><span class="sxs-lookup"><span data-stu-id="a7fd0-187">Tune hello spout</span></span>

<span data-ttu-id="a7fd0-188">Hello ayarları tootune hello spout aşağıdaki değiştirebilirsiniz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-188">You can modify hello following settings tootune hello spout.</span></span>

- <span data-ttu-id="a7fd0-189">**Tuple zaman aşımı: topology.message.timeout.secs**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-189">**Tuple timeout: topology.message.timeout.secs**.</span></span> <span data-ttu-id="a7fd0-190">Bu ayar, hello bir ileti gereken süreyi toocomplete belirler ve kabul edilmeden önce onay, almak başarısız oldu.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-190">This setting determines hello amount of time a message takes toocomplete, and receive acknowledgement, before it is considered failed.</span></span>

- <span data-ttu-id="a7fd0-191">**Çalışan işlemi başına maksimum bellek: worker.childopts**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-191">**Max memory per worker process: worker.childopts**.</span></span> <span data-ttu-id="a7fd0-192">Bu ayarı ek komut satırı parametreleri toohello Java çalışanları belirtmenize olanak sağlar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-192">This setting lets you specify additional command-line parameters toohello Java workers.</span></span> <span data-ttu-id="a7fd0-193">en yaygın olarak kullanılan hello ayarı burada hello ayrılan en fazla bellek tooa JVM'ın yığın belirler XmX ' dir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-193">hello most commonly used setting here is XmX, which determines hello maximum memory allocated tooa JVM’s heap.</span></span>

- <span data-ttu-id="a7fd0-194">**Max spout bekleyen: topology.max.spout.pending**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-194">**Max spout pending: topology.max.spout.pending**.</span></span> <span data-ttu-id="a7fd0-195">Bu ayar hello (henüz onaylanan da hello topoloji tüm düğümlerde) uçuş spout iş parçacığı başına herhangi bir zamanda buna olabilir başlıkların sayısını belirler.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-195">This setting determines hello number of tuples that can in be flight (not yet acknowledged at all nodes in hello topology) per spout thread at any time.</span></span>

 <span data-ttu-id="a7fd0-196">İyi hesaplama toodo tooestimate hello her, başlık boyutudur.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-196">A good calculation toodo is tooestimate hello size of each of your tuples.</span></span> <span data-ttu-id="a7fd0-197">Sonra ne kadar bellek bir spout iş parçacığı olduğunu göstermektedir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-197">Then figure out how much memory one spout thread has.</span></span> <span data-ttu-id="a7fd0-198">Merhaba tooa iş parçacığı, bu değer ile bölünmüş ayrılan toplam bellek size hello üst sınırı için hello max spout parametre bekliyor.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-198">hello total memory allocated tooa thread, divided by this value, should give you hello upper bound for hello max spout pending parameter.</span></span>

## <a name="tune-hello-bolt"></a><span data-ttu-id="a7fd0-199">Merhaba Cıvata ayarlama</span><span class="sxs-lookup"><span data-stu-id="a7fd0-199">Tune hello bolt</span></span>
<span data-ttu-id="a7fd0-200">TooData Lake Store yazarken boyutu eşitleme ilkesi (Merhaba istemci tarafı arabelleği) ayarlamak too4 MB.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-200">When you're writing tooData Lake Store, set a size sync policy (buffer on hello client side) too4 MB.</span></span> <span data-ttu-id="a7fd0-201">Yalnızca hello arabellek boyutu bu değerde hello olduğunda bir temizleme veya hsync() sonra gerçekleştirilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-201">A flushing or hsync() is then performed only when hello buffer size is hello at this value.</span></span> <span data-ttu-id="a7fd0-202">açıkça bir hsync() gerçekleştirmediğiniz sürece hello Data Lake Store sürücü hello çalışan VM üzerinde otomatik olarak bu arabelleğe yapar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-202">hello Data Lake Store driver on hello worker VM automatically does this buffering, unless you explicitly perform an hsync().</span></span>

<span data-ttu-id="a7fd0-203">Merhaba varsayılan Data Lake deposu Storm Cıvata kullanılan tootune olabilecek bir boyutu eşitleme ilkesi parametre (fileBufferSize) Bu parametre içeriyor.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-203">hello default Data Lake Store Storm bolt has a size sync policy parameter (fileBufferSize) that can be used tootune this parameter.</span></span>

<span data-ttu-id="a7fd0-204">İsteğe bağlı olarak g/Ç kullanımı yoğun topolojide iyi bir fikir toohave olan her Cıvata iş parçacığı, bir dosya döndürme İlkesi (fileRotationSize) tooits kendi dosya ve tooset yazma.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-204">In I/O-intensive topologies, it's a good idea toohave each bolt thread write tooits own file, and tooset a file rotation policy (fileRotationSize).</span></span> <span data-ttu-id="a7fd0-205">Merhaba dosya belirli bir boyuta ulaştığında hello akışı otomatik olarak Temizlenen ve yeni bir dosya yazılır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-205">When hello file reaches a certain size, hello stream is automatically flushed and a new file is written to.</span></span> <span data-ttu-id="a7fd0-206">Dosya boyutu döndürme için önerilen hello 1 GB'tır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-206">hello recommended file size for rotation is 1 GB.</span></span>

### <a name="handle-tuple-data"></a><span data-ttu-id="a7fd0-207">Tuple veri işleme</span><span class="sxs-lookup"><span data-stu-id="a7fd0-207">Handle tuple data</span></span>

<span data-ttu-id="a7fd0-208">Açıkça hello Cıvata tarafından onaylanan kadar Storm bir spout tooa tuple tutar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-208">In Storm, a spout holds on tooa tuple until it is explicitly acknowledged by hello bolt.</span></span> <span data-ttu-id="a7fd0-209">Bir tanımlama grubu hello Cıvata tarafından okunabilir ancak henüz onaylanan değil, Data Lake Store arka uç hello spout kalıcı.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-209">If a tuple has been read by hello bolt but has not been acknowledged yet, hello spout might not have persisted into Data Lake Store back end.</span></span> <span data-ttu-id="a7fd0-210">Bir tanımlama grubu kabul edildikten sonra hello spout hello Cıvata tarafından Kalıcılık güvence altına alınabilir ve hello kaynak verileri içinden okuma ne olursa olsun kaynağından sonra silebilirsiniz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-210">After a tuple is acknowledged, hello spout can be guaranteed persistence by hello bolt, and can then delete hello source data from whatever source it is reading from.</span></span>  

<span data-ttu-id="a7fd0-211">Data Lake Store üzerinde en iyi performans için hello Cıvata sahip 4 MB tanımlama grubu veri arabellek.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-211">For best performance on Data Lake Store, have hello bolt buffer 4 MB of tuple data.</span></span> <span data-ttu-id="a7fd0-212">Ardından Data Lake Store geri son bir 4 MB yazma toohello yazın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-212">Then write toohello Data Lake Store back end as one 4-MB write.</span></span> <span data-ttu-id="a7fd0-213">Merhaba veri başarıyla yazılı toohello deposu sonra (arama hflush()) tarafından hello Cıvata hello veri geri toohello spout kabul.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-213">After hello data has been successfully written toohello store (by calling hflush()), hello bolt can acknowledge hello data back toohello spout.</span></span> <span data-ttu-id="a7fd0-214">Bu hangi hello örnektir sağlanan burada mu Cıvata.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-214">This is what hello example bolt supplied here does.</span></span> <span data-ttu-id="a7fd0-215">Aynı zamanda kabul edilebilir toohold fazla sayıda hello hflush() araması yapılmadan önce tanımlama grupları ve hello olan diziler onaylanır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-215">It is also acceptable toohold a larger number of tuples before hello hflush() call is made and hello tuples acknowledged.</span></span> <span data-ttu-id="a7fd0-216">Ancak, bu hello hello spout toohold gerekir ve bu nedenle artar JVM gerekli bellek miktarını hello yürütülen başlıkların sayısını artırır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-216">However, this increases hello number of tuples in flight that hello spout needs toohold, and therefore increases hello amount of memory required per JVM.</span></span>

> [!NOTE]
<span data-ttu-id="a7fd0-217">Uygulamaların diğer olmayan performans nedenleriyle bir gereksinim tooacknowledge başlıkları daha sık (adresindeki veri boyutları'dan 4 MB) olabilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-217">Applications might have a requirement tooacknowledge tuples more frequently (at data sizes less than 4 MB) for other non-performance reasons.</span></span> <span data-ttu-id="a7fd0-218">Ancak, hello g/ç işleme toohello depolama arka ucu etkileyebilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-218">However, that might affect hello I/O throughput toohello storage back end.</span></span> <span data-ttu-id="a7fd0-219">Bu kolaylığını hello Cıvata'nın g/ç performansı karşı dikkatle tartmanız.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-219">Carefully weigh this tradeoff against hello bolt’s I/O performance.</span></span>

<span data-ttu-id="a7fd0-220">Uzun süre toofill Hello 4 MB'lık arabelleğe alır şekilde hello gelen oran başlık değil, yüksekse, bu azaltıcı göz önünde bulundurun:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-220">If hello incoming rate of tuples is not high, so hello 4-MB buffer takes a long time toofill, consider mitigating this by:</span></span>
* <span data-ttu-id="a7fd0-221">Cıvatalar Hello sayısının azaltılması, bu nedenle vardır daha az arabellek toofill.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-221">Reducing hello number of bolts, so there are fewer buffers toofill.</span></span>
* <span data-ttu-id="a7fd0-222">Bir hflush() olduğu bir saat veya sayısı tabanlı ilke sahip her Boşaltılma x veya her y milisaniye tetiklenir ve o ana kadarki birikmiş hello diziler geri onaylanır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-222">Having a time-based or count-based policy, where an hflush() is triggered every x flushes or every y milliseconds, and hello tuples accumulated so far are acknowledged back.</span></span>

<span data-ttu-id="a7fd0-223">Merhaba verimlilik bu durumda düşüktür, ancak olayları yavaş oranı ile en yüksek verimlilik hello büyük hedefi yine de değil unutmayın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-223">Note that hello throughput in this case is lower, but with a slow rate of events, maximum throughput is not hello biggest objective anyway.</span></span> <span data-ttu-id="a7fd0-224">Bu Azaltıcı Etkenler bir tanımlama grubu tooflow toohello deposu üzerinden geçen toplam süre hello azaltmanıza yardımcı olur.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-224">These mitigations help you reduce hello total time that it takes for a tuple tooflow through toohello store.</span></span> <span data-ttu-id="a7fd0-225">Düşük olay hızı bile ile gerçek zamanlı bir işlem hattı istiyorsanız bu önemli.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-225">This might matter if you want a real-time pipeline even with a low event rate.</span></span> <span data-ttu-id="a7fd0-226">Ayrıca, gelen tanımlama grubu hızı düşükse, bunlar alma sırasında zaman aşımı hello diziler olmayan şekilde, hello topology.message.timeout_secs parametre olarak ayarlaması gerektiğini Not arabelleğe veya işlenen.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-226">Also note that if your incoming tuple rate is low, you should adjust hello topology.message.timeout_secs parameter, so hello tuples don’t time out while they are getting buffered or processed.</span></span>

## <a name="monitor-your-topology-in-storm"></a><span data-ttu-id="a7fd0-227">Topolojiniz Storm içindeki izleme</span><span class="sxs-lookup"><span data-stu-id="a7fd0-227">Monitor your topology in Storm</span></span>  
<span data-ttu-id="a7fd0-228">Topolojiniz çalışırken hello Storm kullanıcı arabiriminde izleyebilirsiniz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-228">While your topology is running, you can monitor it in hello Storm user interface.</span></span> <span data-ttu-id="a7fd0-229">Merhaba ana parametreleri toolook adresindeki şunlardır:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-229">Here are hello main parameters toolook at:</span></span>

* <span data-ttu-id="a7fd0-230">**Toplam işlem yürütme gecikme süresi.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-230">**Total process execution latency.**</span></span> <span data-ttu-id="a7fd0-231">Bir tanımlama grubu hello spout'un yayılan, hello Cıvata tarafından işlenir ve onaylanan toobe hello ortalama süreyi budur.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-231">This is hello average time one tuple takes toobe emitted by hello spout, processed by hello bolt, and acknowledged.</span></span>

* <span data-ttu-id="a7fd0-232">**Toplam Cıvata işlem gecikme süresi.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-232">**Total bolt process latency.**</span></span> <span data-ttu-id="a7fd0-233">Bir bildirim alıncaya kadar hello Cıvata adresindeki hello tanımlama grubu tarafından harcanan hello ortalama süre budur.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-233">This is hello average time spent by hello tuple at hello bolt until it receives an acknowledgement.</span></span>

* <span data-ttu-id="a7fd0-234">**Toplam Cıvata gecikme yürütün.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-234">**Total bolt execute latency.**</span></span> <span data-ttu-id="a7fd0-235">Bu hello ortalamadır hello içinde hello Cıvata tarafından harcanan süre yürütme yöntemi.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-235">This is hello average time spent by hello bolt in hello execute method.</span></span>

* <span data-ttu-id="a7fd0-236">**Başarısızlık sayısı.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-236">**Number of failures.**</span></span> <span data-ttu-id="a7fd0-237">Bu, bunlar zaman aşımına uğramadan önce tam olarak işlenen toobe başarısız başlıkların toohello sayısını ifade eder.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-237">This refers toohello number of tuples that failed toobe fully processed before they timed out.</span></span>

* <span data-ttu-id="a7fd0-238">**Kapasitesi.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-238">**Capacity.**</span></span> <span data-ttu-id="a7fd0-239">Bu, sisteminizi ne kadar meşgul olduğundan bir ölçüsüdür.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-239">This is a measure of how busy your system is.</span></span> <span data-ttu-id="a7fd0-240">Bu sayı 1 ise, Cıvatalar yapabilir kadar hızlı çalışmaktadır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-240">If this number is 1, your bolts are working as fast as they can.</span></span> <span data-ttu-id="a7fd0-241">1'den küçük, hello paralellik artırın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-241">If it is less than 1, increase hello parallelism.</span></span> <span data-ttu-id="a7fd0-242">1'den büyükse, hello paralellik azaltın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-242">If it is greater than 1, reduce hello parallelism.</span></span>

## <a name="troubleshoot-common-problems"></a><span data-ttu-id="a7fd0-243">Sık karşılaşılan sorunları giderme</span><span class="sxs-lookup"><span data-stu-id="a7fd0-243">Troubleshoot common problems</span></span>
<span data-ttu-id="a7fd0-244">Birkaç genel sorun giderme senaryoları şunlardır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-244">Here are a few common troubleshooting scenarios.</span></span>
* <span data-ttu-id="a7fd0-245">**Birçok diziler zaman aşımına uğruyor.** Merhaba topoloji toodetermine içindeki her bir düğümün hello performans sorunu olduğu bakın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-245">**Many tuples are timing out.** Look at each node in hello topology toodetermine where hello bottleneck is.</span></span> <span data-ttu-id="a7fd0-246">Merhaba en yaygın nedeni bu hello Cıvatalar mümkün tookeep yukarı hello spout'lar olmamasıdır.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-246">hello most common reason for this is that hello bolts are not able tookeep up with hello spouts.</span></span> <span data-ttu-id="a7fd0-247">Bu, işlenen bekleme toobe sırasında hello iç arabellek tıkamasını tootuples yol açar.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-247">This leads tootuples clogging hello internal buffers while waiting toobe processed.</span></span> <span data-ttu-id="a7fd0-248">Merhaba zaman aşımı değerini artırmayı veya bekleyen hello max spout azaltarak göz önünde bulundurun.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-248">Consider increasing hello timeout value or decreasing hello max spout pending.</span></span>

* <span data-ttu-id="a7fd0-249">**Yüksek toplam işlem yürütme gecikmesi, ancak düşük Cıvata işlem gecikme yoktur.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-249">**There is a high total process execution latency, but a low bolt process latency.**</span></span> <span data-ttu-id="a7fd0-250">Bu durumda, hello diziler yeterince hızlı alınıyor değil, mümkündür.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-250">In this case, it is possible that hello tuples are not being acknowledged fast enough.</span></span> <span data-ttu-id="a7fd0-251">Acknowledgers yeterli sayıda olup olmadığını denetleyin.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-251">Check that there are a sufficient number of acknowledgers.</span></span> <span data-ttu-id="a7fd0-252">Başka bir olasılık hello işlemeden başlangıç Cıvatalar önce bunlar sırada hello çok uzun süre beklediğini olabilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-252">Another possibility is that they are waiting in hello queue for too long before hello bolts start processing them.</span></span> <span data-ttu-id="a7fd0-253">Merhaba max spout bekleyen azaltın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-253">Decrease hello max spout pending.</span></span>

* <span data-ttu-id="a7fd0-254">**Yüksek Cıvata yürütme gecikme süresi yok.**</span><span class="sxs-lookup"><span data-stu-id="a7fd0-254">**There is a high bolt execute latency.**</span></span> <span data-ttu-id="a7fd0-255">Başka bir deyişle, Cıvata hello execute() yöntemi çok uzun sürüyor.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-255">This means that hello execute() method of your bolt is taking too long.</span></span> <span data-ttu-id="a7fd0-256">Merhaba kodu en iyi duruma veya yazma boyutlarda görünüş ve davranışı temizleme.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-256">Optimize hello code, or look at write sizes and flush behavior.</span></span>

### <a name="data-lake-store-throttling"></a><span data-ttu-id="a7fd0-257">Data Lake Store azaltma</span><span class="sxs-lookup"><span data-stu-id="a7fd0-257">Data Lake Store throttling</span></span>
<span data-ttu-id="a7fd0-258">Data Lake Store tarafından sağlanan bant genişliğinin hello sınırına ulaşıp görev hataları görebilirsiniz.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-258">If you hit hello limits of bandwidth provided by Data Lake Store, you might see task failures.</span></span> <span data-ttu-id="a7fd0-259">Hataları azaltma için görev günlüklerini denetleyin.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-259">Check task logs for throttling errors.</span></span> <span data-ttu-id="a7fd0-260">Kapsayıcı boyutu artırarak hello paralellik düşürebilir.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-260">You can decrease hello parallelism by increasing container size.</span></span>    

<span data-ttu-id="a7fd0-261">Kısıtlanan, toocheck hello hata ayıklama hello istemci tarafında günlüğü etkinleştir:</span><span class="sxs-lookup"><span data-stu-id="a7fd0-261">toocheck if you are getting throttled, enable hello debug logging on hello client side:</span></span>

1. <span data-ttu-id="a7fd0-262">İçinde **Ambari** > **Storm** > **Config** > **storm çalışan log4j Gelişmiş**, değiştirme  **&lt;kök düzeyi "bilgi" =&gt;**  çok**&lt;kök düzeyi "hata ayıklama" =&gt;**.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-262">In **Ambari** > **Storm** > **Config** > **Advanced storm-worker-log4j**, change **&lt;root level="info"&gt;** too**&lt;root level=”debug”&gt;**.</span></span> <span data-ttu-id="a7fd0-263">Merhaba yapılandırma tootake etkisi için tüm hello düğümleri/hizmetini yeniden başlatın.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-263">Restart all hello nodes/service for hello configuration tootake effect.</span></span>
2. <span data-ttu-id="a7fd0-264">İzleyici hello Storm topolojisini günlüklerini çalışan düğümlerine (/var/log/storm/worker-artifacts altında /&lt;TopologyName&gt;/&lt;bağlantı noktası&gt;/worker.log) özel durumları azaltma Data Lake Store için.</span><span class="sxs-lookup"><span data-stu-id="a7fd0-264">Monitor hello Storm topology logs on worker nodes (under /var/log/storm/worker-artifacts/&lt;TopologyName&gt;/&lt;port&gt;/worker.log) for Data Lake Store throttling exceptions.</span></span>

## <a name="next-steps"></a><span data-ttu-id="a7fd0-265">Sonraki adımlar</span><span class="sxs-lookup"><span data-stu-id="a7fd0-265">Next steps</span></span>
<span data-ttu-id="a7fd0-266">Storm olarak başvurulabilir için ek performans ayarlaması [bu blog](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-266">Additional performance tuning for Storm can be referenced in [this blog](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/).</span></span>

<span data-ttu-id="a7fd0-267">Ek örnek toorun için bkz: [github'daki bu bir](https://github.com/hdinsight/storm-performance-automation).</span><span class="sxs-lookup"><span data-stu-id="a7fd0-267">For an additional example toorun, see [this one on GitHub](https://github.com/hdinsight/storm-performance-automation).</span></span>
